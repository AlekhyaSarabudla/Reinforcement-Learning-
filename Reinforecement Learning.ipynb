{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Learning_Project_3 (1).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEtBvJvjHnyh"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import gym\n",
        "from gym import spaces\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Stock Trading Environment.\n",
        "\"\"\"DON'T MAKE ANY CHANGES TO THE ENVIRONMENT.\"\"\"\n",
        "\n",
        "\n",
        "class StockTradingEnvironment(gym.Env):\n",
        "    \"\"\"This class implements the Stock Trading environment.\"\"\"\n",
        "\n",
        "    def __init__(self, file_path, train=True, number_of_days_to_consider=10):\n",
        "        \"\"\"This method initializes the environment.\n",
        "\n",
        "        :param file_path: - Path of the CSV file containing the historical stock data.\n",
        "        :param train: - Boolean indicating whether the goal is to train or test the performance of the agent.\n",
        "        :param number_of_days_to_consider = Integer representing whether the number of days the for which the agent\n",
        "                considers the trend in stock price to make a decision.\"\"\"\n",
        "\n",
        "        self.file_path = file_path  # Path of the CSV file containing the historical stock data.\n",
        "        self.stock_data = pd.read_csv(self.file_path)  # Reading the CSV file containing the historical stock data.\n",
        "        self.train = train  # Boolean indicating to use the training stock data by default.\n",
        "        # Splitting the data into train and test datasets.\n",
        "        self.training_stock_data = self.stock_data.iloc[:int(0.8 * len(self.stock_data))]\n",
        "        self.testing_stock_data = self.stock_data.iloc[int(0.8 * len(self.stock_data)):].reset_index()\n",
        "        self.observation_space = spaces.Discrete(4)  # This defines that there are four states in the environment.\n",
        "        # This defines that there are 3 discrete actions that the agent can perform (Buy, Sell, Hold).\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        self.investment_capital = 100000  # This defines the investment capital that the agent starts with.\n",
        "        self.number_of_shares = 0  # This defines number of shares currently held by the agent.\n",
        "        self.stock_value = 0  # This defines the value of the stock currently held by the agent.\n",
        "        self.book_value = 0  # This defines the total value for which the agent bought the shares.\n",
        "        # This defines the agent's total account value.\n",
        "        self.total_account_value = self.investment_capital + self.stock_value\n",
        "        # List to store the total account value over training or evaluation.\n",
        "        self.total_account_value_list = []\n",
        "        # This defines the number of days for which the agent considers the data before taking an action.\n",
        "        self.number_of_days_to_consider = number_of_days_to_consider\n",
        "        # The maximum timesteps the agent will take before the episode ends.\n",
        "        if self.train:\n",
        "            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider\n",
        "        else:\n",
        "            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider\n",
        "        # Initializing the number of steps taken to 0.\n",
        "        self.timestep = 0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"This method resets the environment and returns the observation.\n",
        "\n",
        "        :returns observation: - (Integer in the range of 0 to 3 representing the four possible observations that the\n",
        "                                 agent can receive. The observation depends upon whether the price increased on average\n",
        "                                 in the number of days the agent considers, and whether the agent already has the stock\n",
        "                                 or not.)\"\"\"\n",
        "\n",
        "        self.investment_capital = 100000  # This defines the investment capital that the agent starts with.\n",
        "        self.number_of_shares = 0  # This defines number of shares currently held by the agent.\n",
        "        self.stock_value = 0  # This defines the value of the stock currently held by the agent.\n",
        "        self.book_value = 0  # This defines the total value for which the agent bought the shares.\n",
        "        # This defines the agent's total account value.\n",
        "        self.total_account_value = self.investment_capital + self.stock_value\n",
        "        # List to store the total account value over training or evaluation.\n",
        "        self.total_account_value_list = []\n",
        "        # Initializing the number of steps taken to 0.\n",
        "        self.timestep = 0\n",
        "\n",
        "        # Getting the observation vector.\n",
        "        if self.train:\n",
        "            # If the task is to train the agent the maximum timesteps will be equal to the number of days considered\n",
        "            # subtracted from the  length of the training stock data.\n",
        "            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider\n",
        "\n",
        "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
        "            # considers.\n",
        "            price_increase_list = []\n",
        "            for i in range(self.number_of_days_to_consider):\n",
        "                if self.training_stock_data['Close'][self.timestep + 1 + i] \\\n",
        "                        - self.training_stock_data['Close'][self.timestep + i] > 0:\n",
        "                    price_increase_list.append(1)\n",
        "                else:\n",
        "                    price_increase_list.append(0)\n",
        "\n",
        "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
        "                price_increase = 1\n",
        "                price_decrease = 0\n",
        "            else:\n",
        "                price_increase = 0\n",
        "                price_decrease = 1\n",
        "\n",
        "            # Observation vector that will be passed to the agent.\n",
        "            observation = [price_increase, price_decrease, 0, 1]\n",
        "\n",
        "        else:\n",
        "            # If the task is to evaluate the trained agent's performance the maximum timesteps will be equal to the\n",
        "            # number of days considered subtracted from the  length of the testing stock data.\n",
        "            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider\n",
        "\n",
        "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
        "            # considers.\n",
        "            price_increase_list = []\n",
        "            for i in range(self.number_of_days_to_consider):\n",
        "                if self.testing_stock_data['Close'][self.timestep + 1 + i] \\\n",
        "                        - self.testing_stock_data['Close'][self.timestep + i] > 0:\n",
        "                    price_increase_list.append(1)\n",
        "                else:\n",
        "                    price_increase_list.append(0)\n",
        "\n",
        "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
        "                price_increase = 1\n",
        "                price_decrease = 0\n",
        "            else:\n",
        "                price_increase = 0\n",
        "                price_decrease = 1\n",
        "\n",
        "            # Observation vector.\n",
        "            observation = [price_increase, price_decrease, 0, 1]\n",
        "        if np.array_equal(observation, [1, 0, 0, 1]):\n",
        "            observation = 0\n",
        "        if np.array_equal(observation, [1, 0, 1, 0]):\n",
        "            observation = 1\n",
        "        if np.array_equal(observation, [0, 1, 0, 1]):\n",
        "            observation = 2\n",
        "        if np.array_equal(observation, [0, 1, 1, 0]):\n",
        "            observation = 3\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"This method implements what happens when the agent takes the action to Buy/Sell/Hold.\n",
        "\n",
        "        :param action: - (Integer in the range 0 to 2 inclusive.)\n",
        "\n",
        "        :returns observation: - (Integer in the range of 0 to 3 representing the four possible observations that the\n",
        "                                 agent can receive. The observation depends upon whether the price increased on average\n",
        "                                 in the number of days the agent considers, and whether the agent already has the stock\n",
        "                                 or not.)\n",
        "                 reward: - (Integer/Float value that's used to measure the performance of the agent.)\n",
        "                 done: - (Boolean describing whether or not the episode has ended.)\n",
        "                 info: - (A dictionary that can be used to provide additional implementation information.)\"\"\"\n",
        "\n",
        "        # We give the agent a penalty for taking actions such as buying a stock when the agent doesn't have the\n",
        "        # investment capital and selling a stock when the agent doesn't have any shares.\n",
        "        penalty = 0\n",
        "\n",
        "        if self.train:\n",
        "            if action == 0:  # Buy\n",
        "                if self.number_of_shares > 0:\n",
        "                    penalty = -10\n",
        "                # Determining the number of shares the agent can buy.\n",
        "                number_of_shares_to_buy = math.floor(self.investment_capital / self.training_stock_data[\n",
        "                    'Open'][self.timestep + self.number_of_days_to_consider])\n",
        "                # Adding to the number of shares the agent has.\n",
        "                self.number_of_shares += number_of_shares_to_buy\n",
        "\n",
        "                # Computing the stock value, book value, investment capital and reward.\n",
        "                if number_of_shares_to_buy > 0:\n",
        "                    self.stock_value +=\\\n",
        "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                        * number_of_shares_to_buy\n",
        "                    self.book_value += \\\n",
        "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider]\\\n",
        "                        * number_of_shares_to_buy\n",
        "                    self.investment_capital -= \\\n",
        "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                        * number_of_shares_to_buy\n",
        "\n",
        "                    reward = 1 + penalty\n",
        "\n",
        "                else:\n",
        "                    # Computing the stock value and reward.\n",
        "                    self.stock_value = \\\n",
        "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                        * self.number_of_shares\n",
        "                    reward = -10\n",
        "\n",
        "            if action == 1:  # Sell\n",
        "                # Computing the investment capital, sell value and reward.\n",
        "                self.investment_capital += \\\n",
        "                    self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                    * self.number_of_shares\n",
        "                sell_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                             * self.number_of_shares\n",
        "\n",
        "                if self.book_value > 0:\n",
        "                    reward = (sell_value - self.book_value) / self.book_value * 100\n",
        "                else:\n",
        "                    reward = -10\n",
        "\n",
        "                self.number_of_shares = 0\n",
        "                self.stock_value = 0\n",
        "                self.book_value = 0\n",
        "\n",
        "            if action == 2:  # Hold\n",
        "                # Computing the stock value and reward.\n",
        "                self.stock_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                                   * self.number_of_shares\n",
        "\n",
        "                if self.book_value > 0:\n",
        "                    reward = (self.stock_value - self.book_value) / self.book_value * 100\n",
        "                else:\n",
        "                    reward = -1\n",
        "\n",
        "        else:\n",
        "            if action == 0:  # Buy\n",
        "                if self.number_of_shares > 0:\n",
        "                    penalty = -10\n",
        "                # Determining the number of shares the agent can buy.\n",
        "                number_of_shares_to_buy = math.floor(self.investment_capital / self.testing_stock_data[\n",
        "                    'Open'][self.timestep + self.number_of_days_to_consider])\n",
        "                # Adding to the number of shares the agent has.\n",
        "                self.number_of_shares += number_of_shares_to_buy\n",
        "\n",
        "                # Computing the stock value, book value, investment capital and reward.\n",
        "                if number_of_shares_to_buy > 0:\n",
        "                    self.stock_value += \\\n",
        "                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                        * number_of_shares_to_buy\n",
        "                    self.book_value += \\\n",
        "                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                        * number_of_shares_to_buy\n",
        "                    self.investment_capital -= \\\n",
        "                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                        * number_of_shares_to_buy\n",
        "\n",
        "                    reward = 1 + penalty\n",
        "\n",
        "                else:\n",
        "                    # Computing the stock value and reward.\n",
        "                    self.stock_value = self.training_stock_data['Open'][\n",
        "                                           self.timestep + self.number_of_days_to_consider] * self.number_of_shares\n",
        "                    reward = -10\n",
        "\n",
        "            if action == 1:  # Sell\n",
        "                # Computing the investment capital, sell value and reward.\n",
        "                self.investment_capital += \\\n",
        "                    self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                    * self.number_of_shares\n",
        "                sell_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                             * self.number_of_shares\n",
        "\n",
        "                if self.book_value > 0:\n",
        "                    reward = (sell_value - self.book_value) / self.book_value * 100\n",
        "                else:\n",
        "                    reward = -10\n",
        "\n",
        "                self.number_of_shares = 0\n",
        "                self.stock_value = 0\n",
        "                self.book_value = 0\n",
        "\n",
        "            if action == 2:  # Hold\n",
        "                # Computing the stock value and reward.\n",
        "                self.stock_value = self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
        "                                   * self.number_of_shares\n",
        "\n",
        "                if self.book_value > 0:\n",
        "                    reward = (self.stock_value - self.book_value) / self.book_value * 100\n",
        "                else:\n",
        "                    reward = -1\n",
        "\n",
        "        # Determining if the agent currently has shares of the stock or not.\n",
        "        if self.number_of_shares > 0:\n",
        "            stock_held = 1\n",
        "            stock_not_held = 0\n",
        "        else:\n",
        "            stock_held = 0\n",
        "            stock_not_held = 1\n",
        "\n",
        "        # Getting the observation vector.\n",
        "        if self.train:\n",
        "            # If the task is to train the agent the maximum timesteps will be equal to the number of days considered\n",
        "            # subtracted from the  length of the training stock data.\n",
        "            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider\n",
        "\n",
        "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
        "            # considers.\n",
        "            price_increase_list = []\n",
        "            for i in range(self.number_of_days_to_consider):\n",
        "                if self.training_stock_data['Close'][self.timestep + 1 + i] \\\n",
        "                        - self.training_stock_data['Close'][self.timestep + i] > 0:\n",
        "                    price_increase_list.append(1)\n",
        "                else:\n",
        "                    price_increase_list.append(0)\n",
        "\n",
        "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
        "                price_increase = 1\n",
        "                price_decrease = 0\n",
        "            else:\n",
        "                price_increase = 0\n",
        "                price_decrease = 1\n",
        "\n",
        "            # Observation vector.\n",
        "            observation = [price_increase, price_decrease, stock_held, stock_not_held]\n",
        "\n",
        "        else:\n",
        "            # If the task is to evaluate the trained agent's performance the maximum timesteps will be equal to the\n",
        "            # number of days considered subtracted from the  length of the testing stock data.\n",
        "            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider\n",
        "\n",
        "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
        "            # considers.\n",
        "            price_increase_list = []\n",
        "            for i in range(self.number_of_days_to_consider):\n",
        "                if self.testing_stock_data['Close'][self.timestep + 1 + i] \\\n",
        "                        - self.testing_stock_data['Close'][self.timestep + i] > 0:\n",
        "                    price_increase_list.append(1)\n",
        "                else:\n",
        "                    price_increase_list.append(0)\n",
        "\n",
        "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
        "                price_increase = 1\n",
        "                price_decrease = 0\n",
        "            else:\n",
        "                price_increase = 0\n",
        "                price_decrease = 1\n",
        "\n",
        "            # Observation vector.\n",
        "            observation = [price_increase, price_decrease, stock_held, stock_not_held]\n",
        "\n",
        "        self.timestep += 1  # Increasing the number of steps taken by the agent by 1.\n",
        "\n",
        "        if np.array_equal(observation, [1, 0, 0, 1]):\n",
        "            observation = 0\n",
        "        if np.array_equal(observation, [1, 0, 1, 0]):\n",
        "            observation = 1\n",
        "        if np.array_equal(observation, [0, 1, 0, 1]):\n",
        "            observation = 2\n",
        "        if np.array_equal(observation, [0, 1, 1, 0]):\n",
        "            observation = 3\n",
        "\n",
        "        # Computing the total account value.\n",
        "        self.total_account_value = self.investment_capital + self.stock_value\n",
        "        # Appending the total account value of the list to plot the graph.\n",
        "        self.total_account_value_list.append(self.total_account_value)\n",
        "\n",
        "        # The episode terminates when the number of infected people becomes greater than 75 % of the population.\n",
        "        done = True if (self.timestep >= self.max_timesteps) \\\n",
        "            else False\n",
        "\n",
        "        info = {}\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"This method renders the agent's total account value over time.\n",
        "\n",
        "        :param mode: 'human' renders to the current display or terminal and returns nothing.\"\"\"\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.plot(self.total_account_value_list, color='lightseagreen', linewidth=7)\n",
        "        plt.xlabel('Days', fontsize=32)\n",
        "        plt.ylabel('Total Account Value', fontsize=32)\n",
        "        plt.title('Total Account Value over Time', fontsize=38)\n",
        "        plt.grid()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "x_EFkIXgHxwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  stock_trading_environment = StockTradingEnvironment('NVDA.csv', number_of_days_to_consider=10)"
      ],
      "metadata": {
        "id": "67aOh18XH8Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearning:\n",
        "    \"\"\"This class implements the Q-learning algorithm.\"\"\"\n",
        "\n",
        "    def __init__(self, environment):\n",
        "        \"\"\"This method instantiates the Q-learning parameters.\n",
        "        \n",
        "        :param environment: - This is the environment which needs to be solved.\"\"\"\n",
        "\n",
        "        self.environment = environment\n",
        "\n",
        "        \"\"\"TO DO: Instantiate the Q-learning parameters.\"\"\"\n",
        "        # Initializing the actions and states\n",
        "      \n",
        "        #Declaring the Q-Matrix\n",
        "        \n",
        "        self.gamma = 0.9\n",
        "        self.epsilon = 100\n",
        "        self.Q_table= np.zeros((4,3))\n",
        "        self.learning_rate = 0.1\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"This method performs the agent training.\"\"\"\n",
        "\n",
        "        \"\"\"TO DO: Implement the Q-learning algorithm and train the agent.\"\"\"\n",
        "        rewards_ = []\n",
        "        epsilon_ = []\n",
        "        # Declaring the no of epochs\n",
        "      \n",
        "        for i in range(1500):\n",
        "          initial_observation = self.environment.reset()\n",
        "          rewards_obtained = 0\n",
        " \n",
        "          while 1:\n",
        "            if self.epsilon <10: # If Epsilon is less than 10 then Exploit\n",
        "              action = np.argmax(self.Q_table[initial_observation,:])\n",
        "            else:                # If Epsilon value is greater than 10 then explore from the action space\n",
        "              action = np.random.randint(0,self.environment.action_space.n)\n",
        "            # Action is passed into the step function and we get new observation\n",
        "            new_observation, reward, break_value_, info = self.environment.step(action)\n",
        "\n",
        "            # Q_table is updated with Bellman equation\n",
        "            self.Q_table[initial_observation,action] = (((1-self.learning_rate)*self.Q_table[initial_observation,action]) + (self.learning_rate)*(reward + self.gamma * np.max(self.Q_table[new_observation])))\n",
        "            # Adding rewards for each episode\n",
        "            rewards_obtained =+ reward\n",
        "\n",
        "            # Updating the intial observation with new_observation\n",
        "            initial_observation = new_observation\n",
        "\n",
        "            \n",
        "            # Breaking the loop\n",
        "            if break_value_ == True:\n",
        "              break\n",
        "          # Updating the Epsilon value\n",
        "          epsilon_.append(self.epsilon)\n",
        "          self.epsilon = self.epsilon-0.05*(self.epsilon)\n",
        "          rewards_.append(rewards_obtained)\n",
        "        print(self.Q_table)\n",
        "        return np.array(epsilon_),np.array(rewards_)\n",
        "               \n",
        "          \n",
        "    def evaluate(self):\n",
        "        \"\"\"This method evaluate the trained agent's performance.\"\"\"\n",
        "\n",
        "        \"\"\"\"TO DO: Evaluate the trained agent's performance by selecting only the greedy/best action in each state.\"\"\"\n",
        "        self.environment.train = False\n",
        "        initial_observation = self.environment.reset()\n",
        "        while 1:\n",
        "          action = np.argmax(self.Q_table[initial_observation,:])\n",
        "         \n",
        "          # Observing the next state using the step method in environment\n",
        "          new_observation, reward, break_value_, info = self.environment.step(action)\n",
        "\n",
        "        \n",
        "          # Updating the observation\n",
        "          initial_observation = new_observation\n",
        "            \n",
        "          # Breaking the loop\n",
        "          if break_value_ == True:\n",
        "            break   \n",
        "        self.environment.render()   \n",
        "        return self.environment.total_account_value \n",
        "\n",
        "    def plot(self,x,y,title,xlabel,ylabel):\n",
        "        \"\"\"This method plots the reward dynamics and epsilon decay.\"\"\"\n",
        "\n",
        "        \"\"\"TO DO: Plot the total reward per episode and epsilon decay when training.\"\"\"\n",
        "        plt.plot(y,x)\n",
        "        plt.xlabel(xlabel)\n",
        "        plt.ylabel(ylabel)\n",
        "        plt.title(title)\n",
        "        plt.show()\n",
        "\n",
        "Q_Object = QLearning(stock_trading_environment)"
      ],
      "metadata": {
        "id": "TA_JlC8BH1RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon_values,rewards_values = Q_Object.train()"
      ],
      "metadata": {
        "id": "Ijjt5cqNBp0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_trading_environment.total_account_value"
      ],
      "metadata": {
        "id": "mXBMIHJ7PN1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "income = Q_Object.evaluate()\n",
        "print(income)"
      ],
      "metadata": {
        "id": "dFwbyOpMBp8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_Object.plot(epsilon_values,np.arange(len(epsilon_values)),\"Epsilon Decay\",\"Episodes\",\"Epsilon\")\n"
      ],
      "metadata": {
        "id": "pYyA1x0HlT--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_Object.plot(rewards_values,np.arange(len(rewards_values)),\"Total rewards Per Episode\",\"Episodes\",\"Rewards\")"
      ],
      "metadata": {
        "id": "D9L6ljqzRj_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_trading_environment.total_account_value "
      ],
      "metadata": {
        "id": "UEf3QXXba6yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_trading_environment.total_account_value"
      ],
      "metadata": {
        "id": "mxz4TB3oBqB0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}